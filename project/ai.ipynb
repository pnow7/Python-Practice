{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a5ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "[데이터 분할 및 클래스 가중치]\n",
      "총 데이터 개수: 40669\n",
      "훈련 데이터 개수: 32535\n",
      "검증 데이터 개수: 4067\n",
      "테스트 데이터 개수: 4067\n",
      "훈련 데이터 라벨 분포: 정상(31306), 스팸(1229)\n",
      "계산된 클래스 가중치 (Tensor): tensor([ 0.5196, 13.2364])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad9006d37984a168e0ce689f9e4c823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--beomi--kcbert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fe4e1c131a4226bbf7cb1f650c0b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac15f95585d4414a582c3b8a49250f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT 데이터셋 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install transformers[torch] accelerate -q\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# --- 1. 데이터 로딩 및 전처리 (이전 코드와 동일) ---\n",
    "data_folder_path = 'C:/Users/admin/Downloads/119.국가기록물_대상_초거대AI_학습을_위한_말뭉치_데이터/3.개방데이터/1.데이터/Training'\n",
    "all_processed_data = []\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "json_file_list = glob.glob(os.path.join(data_folder_path, '**', '*.json'), recursive=True)\n",
    "for file_path in json_file_list:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            data = json.load(f)\n",
    "            if 'data' in data and isinstance(data['data'], list):\n",
    "                for item in data['data']:\n",
    "                    text = None\n",
    "                    is_spam = 0\n",
    "                    if 'instruct_text' in item and 'labels' in item:\n",
    "                        for label in item['labels']:\n",
    "                            if 'level1_type' in label and label['level1_type'] == 7:\n",
    "                                text = item['instruct_text']\n",
    "                                is_spam = 1\n",
    "                                if text:\n",
    "                                    all_processed_data.append({'text': text, 'is_spam': is_spam})\n",
    "                                break\n",
    "                    if not is_spam and 'labels' in item and 'instructs' in item['labels'][0]:\n",
    "                        for instruction in item['labels'][0]['instructs']:\n",
    "                            if 'meta' in instruction:\n",
    "                                for meta_item in instruction['meta']:\n",
    "                                    if meta_item.get('category') == 'question':\n",
    "                                        text = instruction.get('text')\n",
    "                                        if text:\n",
    "                                            all_processed_data.append({'text': text, 'is_spam': 0})\n",
    "                                        break\n",
    "                                if text:\n",
    "                                    break\n",
    "            elif 'Data' in data and isinstance(data['Data'], list):\n",
    "                for item in data['Data']:\n",
    "                    if 'corpus' in item:\n",
    "                        text = item['corpus']\n",
    "                        if text:\n",
    "                            all_processed_data.append({'text': text, 'is_spam': 0})\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "txt_file_list = glob.glob(os.path.join(data_folder_path, '**', '*.txt'), recursive=True)\n",
    "for file_path in txt_file_list:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "            if text_content:\n",
    "                all_processed_data.append({'text': text_content, 'is_spam': 0})\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "combined_df = pd.DataFrame(all_processed_data)\n",
    "combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)\n",
    "combined_df.drop_duplicates(subset=['cleaned_text'], inplace=True)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- 2. 데이터 분할 및 클래스 가중치 계산 ---\n",
    "X = combined_df['cleaned_text']\n",
    "y = combined_df['is_spam']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"[데이터 분할 및 클래스 가중치]\")\n",
    "print(f\"총 데이터 개수: {len(X)}\")\n",
    "print(f\"훈련 데이터 개수: {len(X_train)}\")\n",
    "print(f\"검증 데이터 개수: {len(X_val)}\")\n",
    "print(f\"테스트 데이터 개수: {len(X_test)}\")\n",
    "print(f\"훈련 데이터 라벨 분포: 정상({(y_train == 0).sum()}), 스팸({(y_train == 1).sum()})\")\n",
    "print(f\"계산된 클래스 가중치 (Tensor): {class_weights_tensor}\")\n",
    "\n",
    "# --- 3. BERT 토크나이저 및 데이터셋 준비 ---\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SpamDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = SpamDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = SpamDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "print(\"\\nBERT 데이터셋 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5a1a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT 모델 학습 시작]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6102' max='6102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6102/6102 13:46:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.026637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.017515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.025456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "              [최종 BERT 모델 성능 평가]\n",
      "==================================================\n",
      "Accuracy (정확도): 0.9943\n",
      "Precision (정밀도): 0.9012\n",
      "Recall (재현율): 0.9542\n",
      "F1-Score: 0.9270\n",
      "ROC-AUC: 0.9993\n",
      "--------------------------------------------------\n",
      "정상 메일 오탐률: 0.0041 (0.41%)\n",
      "스팸 미탐률: 0.0458 (4.58%)\n",
      "--------------------------------------------------\n",
      "**목표 기준**\n",
      " - F1: >= 0.92\n",
      " - ROC-AUC: >= 0.97\n",
      " - 정상 메일 오탐률: <= 2.5%\n",
      " - 스팸 미탐률: <= 5%\n",
      "==================================================\n",
      "\n",
      "최종 BERT 모델과 토크나이저를 'final_bert_model' 폴더에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1단계에서 정의된 변수들을 사용합니다.\n",
    "# (train_dataset, val_dataset, test_dataset, class_weights_tensor)\n",
    "\n",
    "# --- 1. BERT 모델 및 학습 설정 ---\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "num_labels = 2\n",
    "\n",
    "class WeightedCELoss(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=self.weight)\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        return self.loss_fn(logits, labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "model.classifier = nn.Linear(model.config.hidden_size, num_labels)\n",
    "model.classifier.bias.data.zero_()\n",
    "model.classifier.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "\n",
    "# --- 2. TrainingArguments 정의 ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# --- 3. Trainer 클래스 정의 및 학습 (수정된 부분) ---\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): # **kwargs 추가\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = WeightedCELoss(class_weights_tensor.to(logits.device))(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"[BERT 모델 학습 시작]\")\n",
    "trainer.train()\n",
    "\n",
    "# --- 4. 모델 성능 평가 (테스트 데이터) ---\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        \n",
    "        predictions.extend(probs.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "y_pred_proba = np.array(predictions)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "y_true = np.array(true_labels)\n",
    "\n",
    "# 평가지표 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "fnr = fn / (fn + tp)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"              [최종 BERT 모델 성능 평가]\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
    "print(f\"Precision (정밀도): {precision:.4f}\")\n",
    "print(f\"Recall (재현율): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"정상 메일 오탐률: {fpr:.4f} ({fpr*100:.2f}%)\")\n",
    "print(f\"스팸 미탐률: {fnr:.4f} ({fnr*100:.2f}%)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"**목표 기준**\")\n",
    "print(f\" - F1: >= 0.92\")\n",
    "print(f\" - ROC-AUC: >= 0.97\")\n",
    "print(f\" - 정상 메일 오탐률: <= 2.5%\")\n",
    "print(f\" - 스팸 미탐률: <= 5%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 모델과 토크나이저 저장\n",
    "model.save_pretrained('./final_bert_model')\n",
    "tokenizer.save_pretrained('./final_bert_model')\n",
    "print(\"\\n최종 BERT 모델과 토크나이저를 'final_bert_model' 폴더에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae37ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 토크나이저를 성공적으로 불러왔습니다.\n",
      "\n",
      "[새로운 문장 테스트 결과]\n",
      "\n",
      "\n",
      "입력 문장: '안녕하세요. 내일 2시에 커피 한 잔 하실래요?'\n",
      "분류 결과: 정상(Normal) (정상: 1.0000, 스팸: 0.0000)\n",
      "\n",
      "입력 문장: '광고: 지금 바로 접속하시면 엄청난 혜택이 쏟아집니다. 무료로 확인해보세요!'\n",
      "분류 결과: 스팸(Spam) (정상: 0.0047, 스팸: 0.9953)\n",
      "\n",
      "입력 문장: '일단 고객 정보가 별로 없어서 프로모션 광고 메일을 보낼 수가 없는데 우리 회사의 주 고객층 사람들 메일 주소를 알아낼 방법이 있나요?'\n",
      "분류 결과: 정상(Normal) (정상: 0.9971, 스팸: 0.0029)\n",
      "\n",
      "입력 문장: '재택 알바! 하루 30분 투자로 월 200만원 보장! 지금 신청하세요!'\n",
      "분류 결과: 정상(Normal) (정상: 1.0000, 스팸: 0.0000)\n",
      "\n",
      "입력 문장: '긴급 안내: 고객님께서는 300만원 당첨되셨습니다. 확인을 위해 링크를 클릭하세요.'\n",
      "분류 결과: 정상(Normal) (정상: 1.0000, 스팸: 0.0000)\n",
      "\n",
      "입력 문장: '안녕하세요. 최근 문의하신 건에 대한 답변입니다. 자세한 내용은 첨부파일 확인 부탁드립니다.'\n",
      "분류 결과: 정상(Normal) (정상: 0.9999, 스팸: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 저장된 모델과 토크나이저 불러오기\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./final_bert_model')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('./final_bert_model')\n",
    "    print(\"모델과 토크나이저를 성공적으로 불러왔습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"저장된 모델 파일이 없거나 오류가 발생했습니다. 학습 코드를 먼저 실행하여 모델을 저장해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# 새로운 문장 예측 함수\n",
    "def predict_spam(new_text):\n",
    "    cleaned_text = clean_text(new_text)\n",
    "    \n",
    "    # 텍스트를 토크나이저로 변환\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        cleaned_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 모델 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        \n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    if prediction == 1:\n",
    "        result = \"스팸(Spam)\"\n",
    "    else:\n",
    "        result = \"정상(Normal)\"\n",
    "\n",
    "    print(f\"\\n입력 문장: '{new_text}'\")\n",
    "    print(f\"분류 결과: {result} (정상: {probabilities[0, 0].item():.4f}, 스팸: {probabilities[0, 1].item():.4f})\")\n",
    "\n",
    "# 다양한 스팸 및 정상 문장 테스트\n",
    "print(\"\\n[새로운 문장 테스트 결과]\\n\")\n",
    "predict_spam(\"안녕하세요. 내일 2시에 커피 한 잔 하실래요?\")\n",
    "predict_spam(\"광고: 지금 바로 접속하시면 엄청난 혜택이 쏟아집니다. 무료로 확인해보세요!\")\n",
    "predict_spam(\"일단 고객 정보가 별로 없어서 프로모션 광고 메일을 보낼 수가 없는데 우리 회사의 주 고객층 사람들 메일 주소를 알아낼 방법이 있나요?\")\n",
    "predict_spam(\"재택 알바! 하루 30분 투자로 월 200만원 보장! 지금 신청하세요!\")\n",
    "predict_spam(\"긴급 안내: 고객님께서는 300만원 당첨되셨습니다. 확인을 위해 링크를 클릭하세요.\")\n",
    "predict_spam(\"안녕하세요. 최근 문의하신 건에 대한 답변입니다. 자세한 내용은 첨부파일 확인 부탁드립니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d0440",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
